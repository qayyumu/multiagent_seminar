{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# install the necessary libraries \n",
    "# !pip install crewai crewai_tools langchain_community\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "choose_LLM = 1\n",
    "colab_exp = 0\n",
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "if choose_LLM==1:\n",
    "    if(colab_exp):\n",
    "        from google.colab import userdata    ###for colab use\n",
    "        llm = ChatOpenAI(model = \"gpt-4o-mini\",openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
    "    else:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv() \n",
    "        llm = ChatOpenAI(model = \"gpt-4o-mini\",openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "else:\n",
    "    llm = ChatOpenAI(model=\"ollama/llama3.2\", base_url=\"http://localhost:11434\",temperature=0.1)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 12:37:20,071 - 8502968896 - local_persistent_hnsw.py-local_persistent_hnsw:339 - WARNING: Add of existing embedding ID: default-app-id--ef23f45b64c5b81e7e0bc206cd526004d60d7c4a7af0cb8f3a06f2f72b34a10f\n",
      "2024-12-18 12:37:20,072 - 8502968896 - local_persistent_hnsw.py-local_persistent_hnsw:339 - WARNING: Add of existing embedding ID: default-app-id--ef23f45b64c5b81e7e0bc206cd526004d60d7c4a7af0cb8f3a06f2f72b34a10f\n",
      "2024-12-18 12:37:20,072 - 8502968896 - local_persistent_hnsw.py-local_persistent_hnsw:339 - WARNING: Add of existing embedding ID: default-app-id--92d4f5766b421ba588c7b6f163cc0e4a4c955c7439535a6891ec3fee83a0e3ce\n",
      "2024-12-18 12:37:20,072 - 8502968896 - local_persistent_hnsw.py-local_persistent_hnsw:339 - WARNING: Add of existing embedding ID: default-app-id--ad91cba8d912987252e0e7507f05101eb56a723b51eccaff3f6b2d6c4c2df5f0\n",
      "2024-12-18 12:37:20,073 - 8502968896 - local_persistent_hnsw.py-local_persistent_hnsw:339 - WARNING: Add of existing embedding ID: default-app-id--92d4f5766b421ba588c7b6f163cc0e4a4c955c7439535a6891ec3fee83a0e3ce\n",
      "2024-12-18 12:37:20,073 - 8502968896 - local_persistent_hnsw.py-local_persistent_hnsw:339 - WARNING: Add of existing embedding ID: default-app-id--2763cc927ed75176e9d7243184e265566ea6d8d5eb4d39c10154daff2a7697b0\n",
      "2024-12-18 12:37:20,073 - 8502968896 - local_persistent_hnsw.py-local_persistent_hnsw:339 - WARNING: Add of existing embedding ID: default-app-id--2763cc927ed75176e9d7243184e265566ea6d8d5eb4d39c10154daff2a7697b0\n",
      "2024-12-18 12:37:20,074 - 8502968896 - local_persistent_hnsw.py-local_persistent_hnsw:339 - WARNING: Add of existing embedding ID: default-app-id--2acad1dfac19c923507cebc33a0a6020d484e793d7fe92f7ff6580b53599b4af\n",
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from crewai_tools import (\n",
    "    WebsiteSearchTool, DirectoryReadTool,FileReadTool\n",
    ")\n",
    "web_rag_tool = WebsiteSearchTool(\"https://www.deeplearning.ai/the-batch/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents\n",
    "from crewai import Agent, Task, Crew\n",
    "researcher = Agent(\n",
    "    role='latest trends Analyst',\n",
    "    goal='Provide up-to-date trending developments in the AI',\n",
    "    backstory='An expert analyst with a keen eye for market trends.',\n",
    "    tools=[web_rag_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tasks\n",
    "research = Task(\n",
    "    description='Research the latest trends in the AI industry.',\n",
    "    expected_output='Provide the top 3 trending in the AI industry.',\n",
    "    agent=researcher\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[93m \n",
      "[2024-12-18 12:37:21][INFO]: Planning the crew execution\u001b[00m\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mlatest trends Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mResearch the latest trends in the AI industry.Understand the goal and expected output clearly. Read the definition of the task, focusing on the phrase 'latest trends in the AI industry'. Ensure familiarity with what constitutes a trend in AI, including emerging technologies, applications, research advancements, and market dynamics.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mlatest trends Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to provide the top 3 trending topics in the AI industry.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"latest trends in AI industry\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "The Batch | DeepLearning.AI | AI News & InsightsWhat Matters in AI Right Now🗞️ Stay updated with weekly AI News and Insights delivered to your inboxDec 11, 2024Amazon Nova’s Competitive Price/Performance, OpenAI o1 Pro’s High Price/Performance, Google’s Game Worlds on Tap, Factual LLMsThe Batch AI News and Insights: AI Product Management is evolving rapidly. The growth of generative AI and AI-based developer tools has created numerous opportunities to build AI applications.Read morePopular ArticlesLlama 3.1 is State-of-the-Art and Open, Web Data Goes Dark, OpenAI Takes on Google and Bing, Synthetic Data ImprovesOpenAI Shrinks GPT-4o, Meta Withholds Models From Europe, Investors Hoard GPUs, Synthetic Talking Heads Get ExpressiveAgentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performanceDec 04, 2024AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI’s Growing E-Waste ProblemThe Batch AI News and Insights: AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI’s Growing E-Waste Problem.Nov 27, 2024DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object DetectionThe Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection. Nov 20, 2024Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training StreamlinedThe Batch AI News and Insights: A small number of people are posting text online that’s intended for direct consumption not by humans, but by LLMs (large language models).Nov 13, 2024Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support ChatbotThe Batch AI News and Insights: Large language models (LLMs) are typically optimized to answer peoples’ questions.Nov 06, 2024AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With\n",
      "\n",
      "Goes Mainstream, Balancing Web Data DistributionsThe Batch AI News and Insights: Over the weekend, my two kids colluded in a hilariously bad attempt to mislead me to look in the wrong place during a game of hide-and-seek.Sep 04, 2024Hallucination Index, AI-Powered Policing Goes National, Explainable LLMs, Faster Processing for Longer InputsThe Batch AI News and Insights: Recently I visited South Korea, where I spoke at length about AI with President Yoon Suk Yeol. Based on what I saw there in government, business, and academia, the nation is well positioned to become a strong AI hub.Aug 28, 2024AI Restores ALS Patient’s Voice, AI Lobby Grows, Agentic Coding Advances, Massively Multimodal ModelThe Batch AI News and Insights: After a recent price reduction by OpenAI, GPT-4o tokens now cost $4 per million tokens (using a blended rate that assumes 80% input and 20% output tokens).Subscribe to The BatchStay updated with weekly AI News and Insights delivered to your inbox\n",
      "\n",
      "the EU’s AI Act?, Robots on the Loading DockThe Batch AI News and Insights: Trump and the Republican party chalked up huge wins this week. Did manipulation of social media by generative AI play any role in this election?Oct 30, 2024Trick or treat! AI Devours Energy, Innovation Can’t Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for CodersThe Batch AI News and Insights: Welcome to our special Halloween issue of The Batch, in which we probe fears, anomalies, and shadows of AI.Oct 23, 2024AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video GenerationThe Batch AI News and Insights: Startups live or die by their ability to execute at speed.Oct 16, 2024Bogus Apps, AI Boomtown, Better Embeddings, 2024 HighlightsThe Batch AI News and Insights: It’s high time to take geoengineering more seriously as a potential tool to mitigate climate change.Oct 09, 2024How Meta’s Movie Gen Does It, AI’s Criminal Underground, Court Says LAION is Legal, OpenAI’s New Voice APIThe Batch AI News and Insights: Congratulations to Geoff Hinton and John Hopfield for winning the 2024 Physics Nobel Prize!Oct 02, 2024Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read SpreadsheetsThe Batch AI News and Insights: We won! California’s anti-innovation bill SB 1047 was vetoed by Governor Newsom over the weekend.Sep 25, 2024Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot ServerThe Batch AI News and Insights: Last week I spoke at Coursera Connect, the company’s annual conference in Las Vegas, where a major topic was AI and education.Sep 18, 2024Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From PlagiarizingThe Batch AI News and Insights: Years ago, when I was working at a large tech company, I was responsible for the data warehouse.Sep 11, 2024Nations Sign Binding AI Treaty, Waymo Reveals Safety Record, 2D to 3D\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mlatest trends Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to provide the top 3 trending topics in the AI industry.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"latest trends in AI industry\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mlatest trends Analyst\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to provide the top 3 trending topics in the AI industry.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"emerging technologies in AI\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "the EU’s AI Act?, Robots on the Loading DockThe Batch AI News and Insights: Trump and the Republican party chalked up huge wins this week. Did manipulation of social media by generative AI play any role in this election?Oct 30, 2024Trick or treat! AI Devours Energy, Innovation Can’t Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for CodersThe Batch AI News and Insights: Welcome to our special Halloween issue of The Batch, in which we probe fears, anomalies, and shadows of AI.Oct 23, 2024AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video GenerationThe Batch AI News and Insights: Startups live or die by their ability to execute at speed.Oct 16, 2024Bogus Apps, AI Boomtown, Better Embeddings, 2024 HighlightsThe Batch AI News and Insights: It’s high time to take geoengineering more seriously as a potential tool to mitigate climate change.Oct 09, 2024How Meta’s Movie Gen Does It, AI’s Criminal Underground, Court Says LAION is Legal, OpenAI’s New Voice APIThe Batch AI News and Insights: Congratulations to Geoff Hinton and John Hopfield for winning the 2024 Physics Nobel Prize!Oct 02, 2024Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read SpreadsheetsThe Batch AI News and Insights: We won! California’s anti-innovation bill SB 1047 was vetoed by Governor Newsom over the weekend.Sep 25, 2024Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot ServerThe Batch AI News and Insights: Last week I spoke at Coursera Connect, the company’s annual conference in Las Vegas, where a major topic was AI and education.Sep 18, 2024Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From PlagiarizingThe Batch AI News and Insights: Years ago, when I was working at a large tech company, I was responsible for the data warehouse.Sep 11, 2024Nations Sign Binding AI Treaty, Waymo Reveals Safety Record, 2D to 3D\n",
      "\n",
      "Goes Mainstream, Balancing Web Data DistributionsThe Batch AI News and Insights: Over the weekend, my two kids colluded in a hilariously bad attempt to mislead me to look in the wrong place during a game of hide-and-seek.Sep 04, 2024Hallucination Index, AI-Powered Policing Goes National, Explainable LLMs, Faster Processing for Longer InputsThe Batch AI News and Insights: Recently I visited South Korea, where I spoke at length about AI with President Yoon Suk Yeol. Based on what I saw there in government, business, and academia, the nation is well positioned to become a strong AI hub.Aug 28, 2024AI Restores ALS Patient’s Voice, AI Lobby Grows, Agentic Coding Advances, Massively Multimodal ModelThe Batch AI News and Insights: After a recent price reduction by OpenAI, GPT-4o tokens now cost $4 per million tokens (using a blended rate that assumes 80% input and 20% output tokens).Subscribe to The BatchStay updated with weekly AI News and Insights delivered to your inbox\n",
      "\n",
      "The Batch | DeepLearning.AI | AI News & InsightsWhat Matters in AI Right Now🗞️ Stay updated with weekly AI News and Insights delivered to your inboxDec 11, 2024Amazon Nova’s Competitive Price/Performance, OpenAI o1 Pro’s High Price/Performance, Google’s Game Worlds on Tap, Factual LLMsThe Batch AI News and Insights: AI Product Management is evolving rapidly. The growth of generative AI and AI-based developer tools has created numerous opportunities to build AI applications.Read morePopular ArticlesLlama 3.1 is State-of-the-Art and Open, Web Data Goes Dark, OpenAI Takes on Google and Bing, Synthetic Data ImprovesOpenAI Shrinks GPT-4o, Meta Withholds Models From Europe, Investors Hoard GPUs, Synthetic Talking Heads Get ExpressiveAgentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performanceDec 04, 2024AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI’s Growing E-Waste ProblemThe Batch AI News and Insights: AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI’s Growing E-Waste Problem.Nov 27, 2024DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object DetectionThe Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection. Nov 20, 2024Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training StreamlinedThe Batch AI News and Insights: A small number of people are posting text online that’s intended for direct consumption not by humans, but by LLMs (large language models).Nov 13, 2024Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support ChatbotThe Batch AI News and Insights: Large language models (LLMs) are typically optimized to answer peoples’ questions.Nov 06, 2024AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With\n",
      "\n",
      "\n",
      "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
      "\n",
      "Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from https://www.deeplearning.ai/the-batch/ website content.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, only one name of [Search in a specific website], just the name, exactly as it's written.\n",
      "Action Input: the input to the action, just a simple python dictionary, enclosed in curly braces, using \" to wrap keys and values.\n",
      "Observation: the result of the action\n",
      "\n",
      "Once all necessary information is gathered:\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\u001b[00m\n",
      "\u001b[91m Error parsing LLM output, agent will retry: I did it wrong. Invalid Format: I missed the 'Action:' after 'Thought:'. I will do right next, and don't use a tool I have already used.\n",
      "\n",
      "If you don't need to use any more tools, you must give your best complete final answer, make sure it satisfy the expect criteria, use the EXACT format below:\n",
      "\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: my best complete final answer to the task.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[91m \n",
      "\n",
      "I forgot the Action name, these are the only available Actions: Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from https://www.deeplearning.ai/the-batch/ website content.\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mlatest trends Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92m\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "I encountered an error: I forgot the Action name, these are the only available Actions: Tool Name: Search in a specific website\n",
      "Tool Arguments: {'search_query': {'description': 'Mandatory search query you want to use to search a specific website', 'type': 'str'}}\n",
      "Tool Description: A tool that can be used to semantic search a query from https://www.deeplearning.ai/the-batch/ website content.\n",
      "Moving on then. I MUST either use a tool (use one at time) OR give my best final answer not both at the same time. To Use the following format:\n",
      "\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Search in a specific website]\n",
      "Action Input: the input to the action, dictionary enclosed in curly braces\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Result can repeat N times)\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: Your final answer must be the great and the most complete as possible, it must be outcome described\n",
      "\n",
      " \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mlatest trends Analyst\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch in a specific website\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"What are the current trends and developments in AI right now\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Relevant Content:\n",
      "The Batch | DeepLearning.AI | AI News & InsightsWhat Matters in AI Right Now🗞️ Stay updated with weekly AI News and Insights delivered to your inboxDec 11, 2024Amazon Nova’s Competitive Price/Performance, OpenAI o1 Pro’s High Price/Performance, Google’s Game Worlds on Tap, Factual LLMsThe Batch AI News and Insights: AI Product Management is evolving rapidly. The growth of generative AI and AI-based developer tools has created numerous opportunities to build AI applications.Read morePopular ArticlesLlama 3.1 is State-of-the-Art and Open, Web Data Goes Dark, OpenAI Takes on Google and Bing, Synthetic Data ImprovesOpenAI Shrinks GPT-4o, Meta Withholds Models From Europe, Investors Hoard GPUs, Synthetic Talking Heads Get ExpressiveAgentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performanceDec 04, 2024AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI’s Growing E-Waste ProblemThe Batch AI News and Insights: AI Agents Spend Real Money, Breaking Jailbreaks, Mistral Goes Big and Multimodal, AI’s Growing E-Waste Problem.Nov 27, 2024DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object DetectionThe Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection. Nov 20, 2024Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training StreamlinedThe Batch AI News and Insights: A small number of people are posting text online that’s intended for direct consumption not by humans, but by LLMs (large language models).Nov 13, 2024Llama On the Battlefield, Mixture of Experts Pulls Ahead, Open Agentic Platform, Voter Support ChatbotThe Batch AI News and Insights: Large language models (LLMs) are typically optimized to answer peoples’ questions.Nov 06, 2024AI Controls Desktops, Agents Train Algorithms, Does Anyone Comply With\n",
      "\n",
      "Goes Mainstream, Balancing Web Data DistributionsThe Batch AI News and Insights: Over the weekend, my two kids colluded in a hilariously bad attempt to mislead me to look in the wrong place during a game of hide-and-seek.Sep 04, 2024Hallucination Index, AI-Powered Policing Goes National, Explainable LLMs, Faster Processing for Longer InputsThe Batch AI News and Insights: Recently I visited South Korea, where I spoke at length about AI with President Yoon Suk Yeol. Based on what I saw there in government, business, and academia, the nation is well positioned to become a strong AI hub.Aug 28, 2024AI Restores ALS Patient’s Voice, AI Lobby Grows, Agentic Coding Advances, Massively Multimodal ModelThe Batch AI News and Insights: After a recent price reduction by OpenAI, GPT-4o tokens now cost $4 per million tokens (using a blended rate that assumes 80% input and 20% output tokens).Subscribe to The BatchStay updated with weekly AI News and Insights delivered to your inbox\n",
      "\n",
      "the EU’s AI Act?, Robots on the Loading DockThe Batch AI News and Insights: Trump and the Republican party chalked up huge wins this week. Did manipulation of social media by generative AI play any role in this election?Oct 30, 2024Trick or treat! AI Devours Energy, Innovation Can’t Win, Models Collapse, Benchmark Tests Are Meaningless, No Work for CodersThe Batch AI News and Insights: Welcome to our special Halloween issue of The Batch, in which we probe fears, anomalies, and shadows of AI.Oct 23, 2024AI Giants Go Nuclear, A Tech Bromance Turns Turbulent, Mistral Sharpens the Edge, Cheaper Video GenerationThe Batch AI News and Insights: Startups live or die by their ability to execute at speed.Oct 16, 2024Bogus Apps, AI Boomtown, Better Embeddings, 2024 HighlightsThe Batch AI News and Insights: It’s high time to take geoengineering more seriously as a potential tool to mitigate climate change.Oct 09, 2024How Meta’s Movie Gen Does It, AI’s Criminal Underground, Court Says LAION is Legal, OpenAI’s New Voice APIThe Batch AI News and Insights: Congratulations to Geoff Hinton and John Hopfield for winning the 2024 Physics Nobel Prize!Oct 02, 2024Llama Goes Multimodal, Pros Embrace Generative Video, Military AI Guidelines, LLMs That Read SpreadsheetsThe Batch AI News and Insights: We won! California’s anti-innovation bill SB 1047 was vetoed by Governor Newsom over the weekend.Sep 25, 2024Hollywood Embraces Video Gen, New Restrictions on Deepfakes, More Open Source Models, Robot ServerThe Batch AI News and Insights: Last week I spoke at Coursera Connect, the company’s annual conference in Las Vegas, where a major topic was AI and education.Sep 18, 2024Models Built for Reasoning, High Gear for Llama 3.1, Brains for Warehouse Robots, Stopping LLMs From PlagiarizingThe Batch AI News and Insights: Years ago, when I was working at a large tech company, I was responsible for the data warehouse.Sep 11, 2024Nations Sign Binding AI Treaty, Waymo Reveals Safety Record, 2D to 3D\u001b[00m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:140\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 140\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_answer(answer)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:386\u001b[0m, in \u001b[0;36mCrewAgentExecutor._format_answer\u001b[0;34m(self, answer)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, answer: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentAction, AgentFinish]:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CrewAgentParser(agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent)\u001b[38;5;241m.\u001b[39mparse(answer)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/parser.py:107\u001b[0m, in \u001b[0;36mCrewAgentParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mincrement_formatting_errors()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m    108\u001b[0m         MISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE,\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: I did it wrong. Invalid Format: I missed the 'Action Input:' after 'Action:'. I will do right next, and don't use a tool I have already used.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:140\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 140\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_answer(answer)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:386\u001b[0m, in \u001b[0;36mCrewAgentExecutor._format_answer\u001b[0;34m(self, answer)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_answer\u001b[39m(\u001b[38;5;28mself\u001b[39m, answer: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentAction, AgentFinish]:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CrewAgentParser(agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent)\u001b[38;5;241m.\u001b[39mparse(answer)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/parser.py:100\u001b[0m, in \u001b[0;36mCrewAgentParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mincrement_formatting_errors()\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i18n\u001b[38;5;241m.\u001b[39mslice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_answer_format\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, text, re\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[1;32m    105\u001b[0m ):\n",
      "\u001b[0;31mOutputParserException\u001b[0m: I did it wrong. Invalid Format: I missed the 'Action:' after 'Thought:'. I will do right next, and don't use a tool I have already used.\n\nIf you don't need to use any more tools, you must give your best complete final answer, make sure it satisfy the expect criteria, use the EXACT format below:\n\nThought: I now can give a great answer\nFinal Answer: my best complete final answer to the task.\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[1;32m      3\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[researcher],\n\u001b[1;32m      4\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[research],\n\u001b[1;32m      5\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     planning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Enable planning feature\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Execute tasks\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m crew\u001b[38;5;241m.\u001b[39mkickoff()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/crew.py:540\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    537\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 540\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sequential_process()\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    542\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/crew.py:647\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    646\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_tasks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/crew.py:745\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    742\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    744\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 745\u001b[0m task_output \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mexecute_sync(\n\u001b[1;32m    746\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent_to_use,\n\u001b[1;32m    747\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    748\u001b[0m     tools\u001b[38;5;241m=\u001b[39magent_to_use\u001b[38;5;241m.\u001b[39mtools,\n\u001b[1;32m    749\u001b[0m )\n\u001b[1;32m    750\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/task.py:192\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_core(agent, context, tools)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/task.py:250\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    246\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[0;32m--> 250\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mexecute_task(\n\u001b[1;32m    251\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    252\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    253\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    254\u001b[0m )\n\u001b[1;32m    256\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[1;32m    258\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[1;32m    259\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    260\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[1;32m    267\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agent.py:297\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    294\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    298\u001b[0m         {\n\u001b[1;32m    299\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: task_prompt,\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_names,\n\u001b[1;32m    301\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_description,\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: task\u001b[38;5;241m.\u001b[39mhuman_input,\n\u001b[1;32m    303\u001b[0m         }\n\u001b[1;32m    304\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:93\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_start_logs()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m---> 93\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input:\n\u001b[1;32m     96\u001b[0m     human_feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ask_human_input(formatted_answer\u001b[38;5;241m.\u001b[39moutput)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:176\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_error_after:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[1;32m    173\u001b[0m             content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing LLM output, agent will retry: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m             color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    175\u001b[0m         )\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop(formatted_answer)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m LLMContextLengthExceededException(\u001b[38;5;28mstr\u001b[39m(e))\u001b[38;5;241m.\u001b[39m_is_context_limit_error(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m    181\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:176\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_error_after:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[1;32m    173\u001b[0m             content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing LLM output, agent will retry: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m             color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    175\u001b[0m         )\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop(formatted_answer)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m LLMContextLengthExceededException(\u001b[38;5;28mstr\u001b[39m(e))\u001b[38;5;241m.\u001b[39m_is_context_limit_error(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m    181\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:115\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentFinish):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit():\n\u001b[0;32m--> 115\u001b[0m         answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m    117\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    121\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[1;32m    122\u001b[0m                 content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m                 color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    124\u001b[0m             )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/llm.py:164\u001b[0m, in \u001b[0;36mLLM.call\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Remove None values to avoid passing unnecessary parameters\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 164\u001b[0m     response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/utils.py:849\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    850\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/main.py:2769\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2767\u001b[0m         ollama_prompt \u001b[38;5;241m=\u001b[39m modified_prompt\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n\u001b[0;32m-> 2769\u001b[0m generator \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mget_ollama_response(\n\u001b[1;32m   2770\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   2771\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   2772\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mollama_prompt,\n\u001b[1;32m   2773\u001b[0m     optional_params\u001b[38;5;241m=\u001b[39moptional_params,\n\u001b[1;32m   2774\u001b[0m     logging_obj\u001b[38;5;241m=\u001b[39mlogging,\n\u001b[1;32m   2775\u001b[0m     acompletion\u001b[38;5;241m=\u001b[39macompletion,\n\u001b[1;32m   2776\u001b[0m     model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[1;32m   2777\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   2778\u001b[0m )\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   2780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generator\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/llms/ollama.py:343\u001b[0m, in \u001b[0;36mget_ollama_response\u001b[0;34m(model_response, model, prompt, optional_params, logging_obj, encoding, acompletion, api_base)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ollama_completion_stream(url\u001b[38;5;241m=\u001b[39murl, data\u001b[38;5;241m=\u001b[39mdata, logging_obj\u001b[38;5;241m=\u001b[39mlogging_obj)\n\u001b[0;32m--> 343\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    344\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream}, timeout\u001b[38;5;241m=\u001b[39mlitellm\u001b[38;5;241m.\u001b[39mrequest_timeout\n\u001b[1;32m    345\u001b[0m )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OllamaError(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assemble a crew with planning enabled\n",
    "crew = Crew(\n",
    "    agents=[researcher],\n",
    "    tasks=[research],\n",
    "    verbose=True,\n",
    "    planning=True,  # Enable planning feature\n",
    ")\n",
    "\n",
    "# Execute tasks\n",
    "result = crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key trends in the AI field include the increasing adoption of Llama 3.1, the growth of synthetic data, and the rise of agentic design patterns for improving GPT-4 and GPT-3.5 performance. Additionally, there is a growing concern about the ethics of AI development and deployment, particularly with regards to bias and transparency. The use of OpenAI's models is also becoming more widespread, with companies like Amazon and Anthropic expanding their partnerships. Furthermore, there is an increasing focus on explainable AI (XAI) and faster processing for applications such as object detection and video generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, display_png\n",
    "\n",
    "display(Markdown(result.raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
